import torch
import os
import numpy as np
from crl_kino.env.dubin_gym import DubinGym, DubinGymCU

from crl_kino.policy.rl_policy import load_policy, policy_forward
from matplotlib import pyplot as plt
from torch import nn
import pickle
from crl_kino.policy.net import Actor, Critic
from sklearn.model_selection import train_test_split
def data_collection():
    # collect data generated by local planner
    model_path =os.path.join('log', 'dubin', 'ddpg/policy.pth')


    obs_list = []
    cost_list = []

    

    env = DubinGymCU()

    policy = load_policy(env, [1024, 512, 512, 512], model_path)



    for idx in range(1000):
        print("sampling: {}".format(idx))

        obs = env.reset(2, 10)

        #env.render()

        obs_epi = []
        obs_epi.append(obs)

        t = 0
        while True:
            action = policy_forward(policy, obs, eps=0.0)
            obs, reward, done, info = env.step(action[0])
            #env.render()
            obs_epi.append(obs)

            t += 1

            if done:
                break
        

        if not info['goal']:
            print("not arrive")
            costs = np.zeros([t+1, ])
        else:
            costs = np.flip(np.arange(t+1))
            print("arrive")
        print(costs)
        obs_list.append(np.array(obs_epi))
        cost_list.append(costs*0.2)
    
    obs_np = np.concatenate(obs_list)
    costs_np = np.concatenate(cost_list)
    np.save("obs.npy", obs_np)
    np.save("costs.npy", costs_np)
    return obs_np, costs_np



import argparse

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str, default='log/estimator')
    parser.add_argument('--epoch', type=int, default=500)
    parser.add_argument('--batch_size', type=int, default=128)
    parser.add_argument(
        '--device', type=str,
        default='cuda' if torch.cuda.is_available() else 'cpu')
    args = parser.parse_known_args()[0]
    return args

from crl_kino.estimator.network import TTRCU


import numpy as np
from torch.utils.tensorboard import SummaryWriter

def train(args = get_args()):
    
    obs = np.load('obs.npy')
    cost = np.load('costs.npy')
    ind = cost!=0.0
    obs = obs[ind]
    cost = cost[ind]

    # Create model directory
    if not os.path.exists(args.model_path):
        os.makedirs(args.model_path)

    # Build data loader
    dataset, targets = obs, cost
    assert len(dataset) == len(targets), 'Numbers of instances of inputs and outputs does not match'
    indices = np.arange(len(dataset), dtype=int)
    np.random.shuffle(indices)

    dataset = torch.tensor(dataset[indices], device=args.device, dtype=torch.float)
    targets = torch.tensor(targets[indices], device=args.device, dtype=torch.float)

    # Build the models

    mlp = TTRCU(obs.shape[1], 1, args.device).to(args.device)

    # Loss and Optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adagrad(mlp.parameters())
    
    # Train the Models

    best_epoch = -1
    best_loss = 0.0
    writer = SummaryWriter(log_dir=args.model_path)

    for epoch in range(args.epoch):
        print("epoch {}".format(str(epoch)))

        for i in range(0,len(dataset), args.batch_size):
            # Forward, Backward and Optimize

            # we need zero_grad since pytorch accumulates gradient
            # this is useful in weight sharing like CNN
            mlp.zero_grad()        

            i_ = i+args.batch_size if i+args.batch_size<=len(dataset) else len(dataset)
            dataset_batch, targets_batch = dataset[i:i_], targets[i:i_]
            loss = criterion(mlp(dataset_batch), targets_batch)
            loss.backward()
            optimizer.step()
        loss_train=criterion(mlp(dataset), targets)
        writer.add_scalar('Loss/train', loss_train, epoch)
 
        # Save the models
        if epoch!=0 and epoch%50==0:
            print("loss: {} in #{}".format(loss_train, epoch))
            if best_epoch == -1 or loss_train<best_loss:
                best_loss, best_epoch = loss_train, epoch
            print('best_loss: {} in #{}'.format(best_loss,best_epoch))
            if best_epoch == epoch:
                torch.save(mlp.state_dict(),os.path.join(args.model_path,'estimator.pth'))
    writer.close()
    
if __name__ == "__main__":
    train()